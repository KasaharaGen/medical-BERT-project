import sentencepiece as spm
import pandas as pd
import os
import shutil
from transformers import PreTrainedTokenizerFast

INPUT_CSV = "../data/merged_blocks.csv"
TEXT_COLUMN = "sentence"
CORPUS_PATH = "tokenizer_corpus.txt"
MODEL_PREFIX = "mybert_tokenizer"
MODEL_DIR = "./tokenizer"

os.makedirs(MODEL_DIR, exist_ok=True)

# === CSVã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºãƒ»æ•´å½¢ ===
df = pd.read_csv(INPUT_CSV)
df = df[df[TEXT_COLUMN].notna() & (df[TEXT_COLUMN].str.strip() != "")]
with open(CORPUS_PATH, "w", encoding="utf-8") as f:
    for line in df[TEXT_COLUMN]:
        f.write(line.strip().replace("\n", " ") + "\n")

print(f"ğŸ“„ ã‚³ãƒ¼ãƒ‘ã‚¹æ›¸ãå‡ºã—å®Œäº†: {CORPUS_PATH}")

# === èªå½™ã‚µã‚¤ã‚ºã‚’å‹•çš„ã«èª¿æ•´ï¼ˆæœ€å¤§14981ã«åˆ¶é™ï¼‰ ===
with open(CORPUS_PATH, encoding="utf-8") as f:
    num_lines = sum(1 for _ in f)
VOCAB_SIZE = min(14981, int(num_lines * 2))
print(f"ğŸ”¤ èªå½™ã‚µã‚¤ã‚ºã‚’è¨­å®š: VOCAB_SIZE = {VOCAB_SIZE}")

# === SentencePiece Unigram Tokenizerå­¦ç¿’ ===
spm.SentencePieceTrainer.train(
    input=CORPUS_PATH,
    model_prefix=os.path.join(MODEL_DIR, MODEL_PREFIX),
    vocab_size=VOCAB_SIZE,
    character_coverage=0.9995,
    model_type="unigram",
    bos_id=1,
    eos_id=2,
    unk_id=0,
    pad_id=3,
    user_defined_symbols=["[MASK]"]
)

# === .model ã‚’ tokenizer.model ã«ãƒªãƒãƒ¼ãƒ ã‚³ãƒ”ãƒ¼ ===
shutil.copyfile(f"{MODEL_DIR}/{MODEL_PREFIX}.model", f"{MODEL_DIR}/tokenizer.model")

# === Transformerså½¢å¼ã§ä¿å­˜ ===
tokenizer = PreTrainedTokenizerFast.from_pretrained(
    MODEL_DIR,
    unk_token="<unk>",
    bos_token="<s>",
    eos_token="</s>",
    pad_token="[PAD]",
    mask_token="[MASK]"
)
tokenizer.save_pretrained(MODEL_DIR)
print(f"âœ… Transformerså½¢å¼ã§ä¿å­˜å®Œäº†: {MODEL_DIR}/tokenizer_config.json")
